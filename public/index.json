
[{"content":"","date":"7 August 2025","externalUrl":null,"permalink":"/categories/","section":"Categories","summary":"","title":"Categories","type":"categories"},{"content":"","date":"7 August 2025","externalUrl":null,"permalink":"/categories/poems/","section":"Categories","summary":"","title":"Poems","type":"categories"},{"content":"","date":"7 August 2025","externalUrl":null,"permalink":"/poems/","section":"Poems","summary":"","title":"Poems","type":"poems"},{"content":"","date":"7 August 2025","externalUrl":null,"permalink":"/","section":"Posei Site","summary":"","title":"Posei Site","type":"page"},{"content":"","date":"7 August 2025","externalUrl":null,"permalink":"/posts/","section":"Posts","summary":"","title":"Posts","type":"posts"},{"content":"","date":"7 August 2025","externalUrl":null,"permalink":"/tags/reason/","section":"Tags","summary":"","title":"Reason","type":"tags"},{"content":"","date":"7 August 2025","externalUrl":null,"permalink":"/tags/","section":"Tags","summary":"","title":"Tags","type":"tags"},{"content":"","date":"7 August 2025","externalUrl":null,"permalink":"/categories/thoughts/","section":"Categories","summary":"","title":"Thoughts","type":"categories"},{"content":"You were a spectacular flash,\na streak of lightning across a desperate sky.\nYou were never meant to stay.\nBack then, everything was dazzling.\nThe gushing wind, the fury in every step.\nI ran with a singular heart,\nand I believed that was the height of being.\nI look now at the calm sidewalk.\nThe comfortable, meandering path.\nIt is safe. It is good.\nAnd yet, a heartache washes over me,\na silent question: \u0026ldquo;Is this all?\u0026rdquo;\nFor so long, I thought I saw your back ahead of me,\na ghost I could never surpass.\nI asked myself: What do I lack?\nCourage? Support? Or is it just that my best is behind me?\nI was asking the wrong question.\nThe truth is, I was blinded by the light of that single, brilliant run.\nI failed to see that the glory was never in the speed, or even the finish line.\nIt was in the stumble.\nIt was in the relentless, stubborn, defiant act of getting back up.\nI do not need to surpass you.\nI need to thank you.\nYou were not my rival; you were my proof of concept.\nYou are the living evidence that I can run with a full heart.\nThe curtain on the new era rises.\nIt does not require another lightning strike.\nIt requires the steady, self-generated current of showing up.\nOf trying. Of stumbling. Of rising.\nI am not here to beat the ghost of the best I ever was.\nI am here to build a legacy worthy of the resolve she taught me I had.\nSo I will jog. I will walk. I will sometimes meander.\nBut I will move.\nAnd in that motion, I will find my own wind.\nI stand on the edge of a dream, promising never to turn back.\nNot to you, but toward the horizon you helped me see.\n","date":"7 August 2025","externalUrl":null,"permalink":"/poems/ghost-in-the-starting-gate/","section":"Poems","summary":"\u003cul\u003e\n\u003cli\u003e\u003c/li\u003e\n\u003c/ul\u003e\n","title":"To the Ghost in the Starting Gate","type":"poems"},{"content":"Arima Kinen should be a rather familiar words after the recent horse craze due to the hit game uma musume: pretty derby.\nFor the uninitiated Arima Kinen is a Grade 1 horse race held in Nakayama Racecourse in Japan. For the purpose of this article it is simply a prestigious and popular race, only horses with enough fan votes or prize money won could participate.\nBut, this article is about the horses from the game Uma Musume and not the real world counterpart.\nThe Horse Runs # The Uma Musumes are born to run, that much is very clear, it has been the one fact that is established before every anime or game starts.\nBut soon, each of them is faced with a deeper question: how do they run? With whom do they run? And most importantly, why do they run in the first place?\nSome find their answer quickly. Some struggle. For others, the act of running itself feels like enough, why bother seeking a purpose when the act of running itself already feels like a blessing?\nOguri Cap # This fatty needs no introduction, loved by everyone and she have her own anime!1!1 (go watch Cinderella Gray). She might be a glutton and seems to be afloat sometimes. However, when it comes to racing she transforms into a force of nature.\nHer speed and power is nothing to scoff at, she isn\u0026rsquo;t demolishing monthly cafeteria supply for nothing.\nWin after win, she seems unstoppable\n\u0026ldquo;run a lot, eat a lot, and then all that\u0026rsquo;s left is\u0026hellip; sleep a lot.\u0026rdquo;\nBut when asked why she runs, Oguri struggles. Strength alone isn’t enough, she lacks a clear reason. And though her talent carries her, something feels incomplete. She started to slowly discover her reason, this search for purpose was solidified when she witnesses the Arima Kinen. No grand explanation is needed. Watching the race stirs something indescribable, emotional, heart-pounding, undeniable. In that moment, her resolve crystallizes:\nTo become the strongest.\nTo make her supporters proud.\nTo one day run in the Arima Kinen herself.\nYour Arima Kinen # This isn’t about telling you to watch the Arima Kinen. It’s about bearing witness to the pinnacle of what you care about.\nWhether it’s a competition, a masterpiece, or a finished piece of work, it doesn’t matter. What matters is seeing it at its highest form.\nBecause when you’re uncertain, when your own pursuit of mastery feels shaky, witnessing others give everything at their peak can reignite something in you.\n","date":"7 August 2025","externalUrl":null,"permalink":"/posts/to-watch-the-arima-kinen/","section":"Posts","summary":"The race that might stirs something indescribable, emotional, heart-pounding, undeniable","title":"to watch the arima kinen","type":"posts"},{"content":"i did not just climb a wall.\ni climbed a moment.\na shaking, uncertain moment\ncaught between fear and ascent.\nmy breath trembled,\nmy limbs screamed,\nbut i moved anyway.\nthat wall didn\u0026rsquo;t test my strength\nit revealed it.\nthe fall never came.\ni rose.\nand when i reached the top,\nit wasn\u0026rsquo;t just the height that shook me.\nit was the knowing:\n“something has changed.”\ni didn\u0026rsquo;t just unlock a new skill.\ni unlocked access\nto places in me i didn\u0026rsquo;t know i could reach\nnot just forward, but behind.\ni\u0026rsquo;ve found a door i can return through.\ni\u0026rsquo;ve felt the joy of motion,\nthe glory of effort,\nand the quiet magic of doing it with someone who gets it.\ni was higher than the wall.\nand i\u0026rsquo;m not coming back down the same.\n","date":"25 April 2025","externalUrl":null,"permalink":"/poems/climbing/","section":"Poems","summary":"\u003cul\u003e\n\u003cli\u003e\u003c/li\u003e\n\u003c/ul\u003e\n","title":"Climbing","type":"poems"},{"content":"","date":"10 July 2023","externalUrl":null,"permalink":"/tags/database/","section":"Tags","summary":"","title":"Database","type":"tags"},{"content":"","date":"10 July 2023","externalUrl":null,"permalink":"/tags/performance/","section":"Tags","summary":"","title":"Performance","type":"tags"},{"content":"","date":"10 July 2023","externalUrl":null,"permalink":"/categories/tech-blog/","section":"Categories","summary":"","title":"Tech Blog","type":"categories"},{"content":"The dramatic story (not really) of coping with postgre performance\nThe simplicity mirage # On the 11th of April 2023, a seemingly harmless feature request was thrown into the mix. The team wanted the ability to export sales reports within a specific date range. \u0026ldquo;Piece of cake,\u0026rdquo; I thought confidently. After all, we already had the daily export feature in place. How hard could it be to add a date filter and call it a day?\nOur first attempt at tackling this feature seemed promising. Just add a second date filter and voila! But oh boy, did we get a rude awakening. As soon as we dared to request a month-long sales report, the query simply timed out\u0026hellip;\nThere must be a mistake let\u0026rsquo;s try it again!\nCancelling statement due to statement timeout\nUhh\u0026hellip; Let\u0026rsquo;s try the Supabase sql editor and run the query there\nupstream request timeout\nThe Math isn\u0026rsquo;t Mathing! I mean, come on, if it takes less than a second to fetch the sales for a single day, logic dictates that a month\u0026rsquo;s worth of data would take, what, 30 seconds max?\nInstead of unraveling the enigma at that moment, we collectively decided to take the easy way out and increase the timeout limit. And like magic, we could finally export a month\u0026rsquo;s worth of reports. Victory! Or so we thought\u0026hellip; The feature can only handle 1 month worth of report max\nTaming the elephant # After putting the feature on the back burner and juggling other priorities and projects, I finally mustered the courage to confront the infamous sales report conundrum once again. Determined to find a permanent solution, I dove headfirst into data query.\nUpon examining the current implementation, it became painfully clear that it was ill-equipped to handle anything beyond a measly month of data. As I peered into the depths of the report data view, a sinking feeling washed over me.\nCREATE OR REPLACE VIEW public.\u0026#34;laporanPenjualanDetailNew\u0026#34; AS ( SELECT r.id, r.\u0026#34;isDeleted\u0026#34;, s.id AS \u0026#34;salesId\u0026#34;, s.name AS \u0026#34;salesName\u0026#34;, s.division AS \u0026#34;salesDivision\u0026#34;, s.\u0026#34;clusterId\u0026#34;, c.name AS \u0026#34;clusterName\u0026#34;, o.id AS \u0026#34;outletId\u0026#34;, o.\u0026#34;digiposId\u0026#34;, o.name AS \u0026#34;outletName\u0026#34;, o.\u0026#34;noChip\u0026#34; AS \u0026#34;outletNoChip\u0026#34;, w.\u0026#34;uniqueId\u0026#34; AS \u0026#34;itemOrProgramId\u0026#34;, CASE WHEN rd.\u0026#34;programId\u0026#34; IS NOT NULL THEN \u0026#39;Program\u0026#39;::text ELSE \u0026#39;Fisik\u0026#39;::text END AS \u0026#34;typeItem\u0026#34;, replace( CASE WHEN rd.\u0026#34;programId\u0026#34; IS NOT NULL THEN (rd.program -\u0026gt; \u0026#39;name\u0026#39;::text)::text ELSE (rd.item -\u0026gt; \u0026#39;name\u0026#39;::text)::text END, \u0026#39;\u0026#34;\u0026#39;::text, \u0026#39;\u0026#39;::text) AS \u0026#34;itemName\u0026#34;, count(rd.id) AS \u0026#34;countItem\u0026#34;, CASE WHEN rd.\u0026#34;programId\u0026#34; IS NOT NULL THEN (rd.program -\u0026gt; \u0026#39;priceProgram\u0026#39;::text)::double precision ELSE (rd.item -\u0026gt; \u0026#39;basePrice\u0026#39;::text)::double precision END AS \u0026#34;priceBase\u0026#34;, CASE WHEN rd.\u0026#34;programId\u0026#34; IS NOT NULL THEN (rd.program -\u0026gt; \u0026#39;price\u0026#39;::text)::double precision ELSE (rd.item -\u0026gt; \u0026#39;price\u0026#39;::text)::double precision END AS \u0026#34;priceSell\u0026#34;, sum( CASE WHEN rd.\u0026#34;programId\u0026#34; IS NOT NULL THEN (rd.program -\u0026gt; \u0026#39;priceProgram\u0026#39;::text)::double precision ELSE (rd.item -\u0026gt; \u0026#39;basePrice\u0026#39;::text)::double precision END) AS \u0026#34;priceBaseTotal\u0026#34;, sum( CASE WHEN rd.\u0026#34;programId\u0026#34; IS NOT NULL THEN (rd.program -\u0026gt; \u0026#39;price\u0026#39;::text)::integer ELSE (rd.item -\u0026gt; \u0026#39;price\u0026#39;::text)::integer END) AS \u0026#34;priceSellTotal\u0026#34;, sum( CASE WHEN rd.\u0026#34;programId\u0026#34; IS NOT NULL THEN (rd.program -\u0026gt; \u0026#39;price\u0026#39;::text)::integer ELSE (rd.item -\u0026gt; \u0026#39;price\u0026#39;::text)::integer END)::double precision - sum( CASE WHEN rd.\u0026#34;programId\u0026#34; IS NOT NULL THEN (rd.program -\u0026gt; \u0026#39;price\u0026#39;::text)::integer::double precision - (rd.program -\u0026gt; \u0026#39;margin\u0026#39;::text)::double precision ELSE (rd.item -\u0026gt; \u0026#39;basePrice\u0026#39;::text)::integer::double precision END) AS laba, to_char(( CASE WHEN r.\u0026#34;updatedAt\u0026#34; IS NULL THEN r.\u0026#34;createdAt\u0026#34; ELSE r.\u0026#34;updatedAt\u0026#34; END AT TIME ZONE \u0026#39;ASIA/JAKARTA\u0026#39;::text)::date::timestamp with time zone, \u0026#39;yyyyMMdd\u0026#39;::text)::integer AS \u0026#34;dateInt\u0026#34;, to_char(( CASE WHEN r.\u0026#34;updatedAt\u0026#34; IS NULL THEN r.\u0026#34;createdAt\u0026#34; ELSE r.\u0026#34;updatedAt\u0026#34; END AT TIME ZONE \u0026#39;ASIA/JAKARTA\u0026#39;::text)::date::timestamp with time zone, \u0026#39;yyyy-MM-dd\u0026#39;::text) AS date, (r.\u0026#34;createdAt\u0026#34; AT TIME ZONE \u0026#39;ASIA/JAKARTA\u0026#39;::text) AS \u0026#34;createdAt\u0026#34;, w.\u0026#34;uniqueId\u0026#34; FROM \u0026#34;receiptDetails\u0026#34; rd, receipts r, \u0026#34;masterSales\u0026#34; s, \u0026#34;masterClusters\u0026#34; c, \u0026#34;masterOutlets\u0026#34; o, \u0026#34;warehouseStocks\u0026#34; w WHERE r.id = rd.\u0026#34;receiptId\u0026#34; AND r.\u0026#34;isDeleted\u0026#34; = false AND rd.\u0026#34;isDeleted\u0026#34; = false AND w.\u0026#34;serialNumber\u0026#34; = rd.\u0026#34;serialNumber\u0026#34; AND s.id = r.\u0026#34;salesId\u0026#34; AND o.id = r.\u0026#34;outletId\u0026#34; AND c.id = s.\u0026#34;clusterId\u0026#34; GROUP BY w.\u0026#34;uniqueId\u0026#34;, s.id, r.id, ( CASE WHEN rd.\u0026#34;programId\u0026#34; IS NOT NULL THEN \u0026#39;Program\u0026#39;::text ELSE \u0026#39;Fisik\u0026#39;::text END), (replace( CASE WHEN rd.\u0026#34;programId\u0026#34; IS NOT NULL THEN (rd.program -\u0026gt; \u0026#39;name\u0026#39;::text)::text ELSE (rd.item -\u0026gt; \u0026#39;name\u0026#39;::text)::text END, \u0026#39;\u0026#34;\u0026#39;::text, \u0026#39;\u0026#39;::text)), ( CASE WHEN rd.\u0026#34;programId\u0026#34; IS NOT NULL THEN (rd.program -\u0026gt; \u0026#39;priceProgram\u0026#39;::text)::double precision ELSE (rd.item -\u0026gt; \u0026#39;basePrice\u0026#39;::text)::double precision END), ( CASE WHEN rd.\u0026#34;programId\u0026#34; IS NOT NULL THEN (rd.program -\u0026gt; \u0026#39;price\u0026#39;::text)::double precision ELSE (rd.item -\u0026gt; \u0026#39;price\u0026#39;::text)::double precision END), (to_char(( CASE WHEN r.\u0026#34;updatedAt\u0026#34; IS NULL THEN r.\u0026#34;createdAt\u0026#34; ELSE r.\u0026#34;updatedAt\u0026#34; END AT TIME ZONE \u0026#39;ASIA/JAKARTA\u0026#39;::text)::date::timestamp with time zone, \u0026#39;yyyy-MM-dd\u0026#39;::text)), r.\u0026#34;createdAt\u0026#34;, c.id, o.id ORDER BY r.id) UNION ( SELECT r.id, r.\u0026#34;isDeleted\u0026#34;, s.id AS \u0026#34;salesId\u0026#34;, s.name AS \u0026#34;salesName\u0026#34;, s.division AS \u0026#34;salesDivision\u0026#34;, s.\u0026#34;clusterId\u0026#34;, c.name AS \u0026#34;clusterName\u0026#34;, o.id AS \u0026#34;outletId\u0026#34;, o.\u0026#34;digiposId\u0026#34;, o.name AS \u0026#34;outletName\u0026#34;, o.\u0026#34;noChip\u0026#34; AS \u0026#34;outletNoChip\u0026#34;, \u0026#39;LA\u0026#39;::text AS \u0026#34;itemOrProgramId\u0026#34;, \u0026#39;LA\u0026#39;::text AS \u0026#34;typeItem\u0026#34;, \u0026#39;LA\u0026#39;::text AS \u0026#34;itemName\u0026#34;, 0 AS \u0026#34;countItem\u0026#34;, 0 AS \u0026#34;priceBase\u0026#34;, 0 AS \u0026#34;priceSell\u0026#34;, 0 AS \u0026#34;priceBaseTotal\u0026#34;, r.cash + r.bni + r.bri + r.bca + r.mandiri AS \u0026#34;priceSellTotal\u0026#34;, 0 AS laba, to_char(( CASE WHEN r.\u0026#34;updatedAt\u0026#34; IS NULL THEN r.\u0026#34;createdAt\u0026#34; ELSE r.\u0026#34;updatedAt\u0026#34; END AT TIME ZONE \u0026#39;ASIA/JAKARTA\u0026#39;::text)::date::timestamp with time zone, \u0026#39;yyyyMMdd\u0026#39;::text)::integer AS \u0026#34;dateInt\u0026#34;, to_char(( CASE WHEN r.\u0026#34;updatedAt\u0026#34; IS NULL THEN r.\u0026#34;createdAt\u0026#34; ELSE r.\u0026#34;updatedAt\u0026#34; END AT TIME ZONE \u0026#39;ASIA/JAKARTA\u0026#39;::text)::date::timestamp with time zone, \u0026#39;yyyy-MM-dd\u0026#39;::text) AS date, (r.\u0026#34;createdAt\u0026#34; AT TIME ZONE \u0026#39;ASIA/JAKARTA\u0026#39;::text) AS \u0026#34;createdAt\u0026#34;, \u0026#39;\u0026#39;::text AS \u0026#34;uniqueId\u0026#34; FROM receipts r JOIN \u0026#34;masterSales\u0026#34; s ON s.id = r.\u0026#34;salesId\u0026#34; AND r.\u0026#34;isDeleted\u0026#34; = false JOIN \u0026#34;masterClusters\u0026#34; c ON c.id = s.\u0026#34;clusterId\u0026#34; JOIN \u0026#34;masterOutlets\u0026#34; o ON o.id = r.\u0026#34;outletId\u0026#34; WHERE r.type = \u0026#39;Top Up\u0026#39;::text ORDER BY r.id) ORDER BY 21 DESC; It was a tangled web of complexity, a union of two queries, adorned with branching conditions, spanning across five tables with thousands upon thousands of records. Good grief! Where on earth does one even begin to untangle this intricate mess?\nYou see, the App relies on Supabase as its trusty database provider, with PostgreSQL doing all the heavy lifting behind the scenes. Now, I must confess that while I know a thing or two in NoSQL databases like Firebase, I lack the hours to tackle the world of SQL databases, especially on this scale.\nRealizing my lack of understanding in PostgreSQL and feeling completely lost, I knew I had to get up to speed quickly. So, for the next two nights, I dived into the PostgreSQL documentation headfirst. Although I didn\u0026rsquo;t read every single page, I made sure to grasp the most important concepts and get a good grasp of where to begin.\nWith each section I read, PostgreSQL started to make more sense. I learned about indexing strategies, vacuuming, and the basics of query optimization. It was like finding puzzle pieces that helped me see the bigger picture.\nAfter my intensive (not really) reading session, I felt more confident. While there was still much to learn, I now had a solid starting point. I was ready to take on the challenge armed with my newfound PostgreSQL knowledge.\nFinding half of the solution: Index # Alright, it\u0026rsquo;s time to face the cold, hard truth. Brace yourselves, SQL experts, because here comes the confession: we weren\u0026rsquo;t using indexes. I can almost hear the collective gasps and facepalms echoing in the distance. In my blissful ignorance, I thought everything was hunky-dory.\nBut let\u0026rsquo;s take a moment to understand the significance of indexes. They play a crucial role in optimizing queries. The table is a book without an index you flip through the entire book to find what you\u0026rsquo;re looking for with an index it looks like this:\nTo illustrate the importance of indexes, let\u0026rsquo;s dive into some charts. These charts depict the speed gains achieved after implementing indexes. Prepare to be amazed!\nSmaller is better\nVoila! With the power of indexes, we managed to save a whopping 4 seconds! Now, I know it may not sound like much in the grand scheme of things, but in the world of database queries, that\u0026rsquo;s a significant improvement.\nWhen we used the magical explain analyze command on our query, we witnessed a remarkable transformation. Gone were the days of the dreaded sequential scan where PostgreSQL painstakingly sifted through every page of the book. Instead, we now basked in the glory of the index scan on xx a sight that filled our hearts with joy. It was a clear indication that our indexes were being put to good use.\nHowever, despite our valiant efforts and the progress we had made, we were not out of the woods just yet. As we prepared to run our export feature, a dark cloud of dread loomed overhead. The haunting specter of the \u0026ldquo;Cancelling statement due to statement timeout\u0026rdquo; still haunted us like a persistent ghost. It seemed like our 4-second victory was simply not enough to banish this relentless foe.\nFinding impostor among us # With no other options left, I had to roll up my sleeves and get my hands dirty with the query itself. It was time to play detective and identify the culprit responsible for the sluggish execution time. Was it the sneaky union causing trouble? Or maybe the complex branching logic? Perhaps the seemingly innocent aggregate function had a hidden agenda?\nIn my pursuit of answers, I embarked on a process of elimination. Similar to a game of deduction, I had to eliminate suspects one by one until I found the true culprit. Unlike in \u0026ldquo;Among Us,\u0026rdquo; where ejecting the wrong crewmate results in failure, here it was a matter of narrowing down the possibilities and uncovering the source of our woes.\nAfter conducting intense investigations and pouring over the data, the results were finally in. Behold, the revealing chart that held the key to our query performance:\nIt became glaringly obvious that the culprit behind the sluggish execution was the branching logic. However, something still felt fishy. It struck me that when I removed the branch, I also eliminated access to certain columns, particularly those with the jsonb data type. This discovery prompted a daring experiment: what if we completely removed the jsonb data?\nI executed the altered query and watched as the seconds ticked away. And then, the query time plummeted from 33 seconds to a mere 12 seconds. A massive improvement, indeed! It seemed like the solution was within reach, just remove the jsonb and call it a day. But alas, life is never that simple.\nHere\u0026rsquo;s where the dilemma set in. The jsonb data was crucial to our operations. It allowed us to store item data at the time of the transaction, ensuring that any future changes to the item\u0026rsquo;s price wouldn\u0026rsquo;t affect the report. Removing it entirely was not an option. So, what were we to do now? Despite our deliberate efforts, we seemed trapped in a conundrum with no apparent way out.\nBut let\u0026rsquo;s not lose hope just yet. It\u0026rsquo;s said that in the face of adversity, innovation thrives. Perhaps, just perhaps, a glimmer of a solution awaited us, ready to be discovered and embraced.\nA glimmer of hope # Confronted with the harsh reality that improving the existing query was a near-impossible task, I shifted my focus towards finding a way to work around this limitation. A potential solution emerged in my mind: why not offload the export generation to an asynchronous service running on a separate server? By doing so, even if the process took longer, users could simply check back later and continue using the app without waiting for the export to finish.\nFor the backend server, I opted to use Rust.\nNot because I wanted to evangelize or squeeze every ounce of performance, but rather because I knew that Rust\u0026rsquo;s async runtime, Tokioooooo, offered a hassle-free way to create and spawn asynchronous processes. It seemed like a fitting choice for the task at hand.\nactix_web::rt::spawn(async move { // async process here }) // it\u0026#39;s literally that simple I started building the API service in Rust, praying that this approach would finally prove successful. Before long, I completed the first iteration of the service and put it to the test. Miraculously, it worked!\nI could now export more than a single month\u0026rsquo;s worth of data. I pushed the boundaries and tested it with five months\u0026rsquo; worth of data, and it performed admirably. There was no reason to believe that I couldn\u0026rsquo;t scale it further.\nHowever, as is often the case in our journey, my elation was short-lived. I soon discovered that the chance of timeouts and failures still loomed over the process, occurring more often than I had hoped. The initial victory was dampened by this realization, leaving me once again in search of a solution to this persistent challenge\u0026hellip;\nWork around that took out production # In my quest to overcome the timeouts and failures, I devised a workaround. First, I resorted to an extreme measure by increasing the timeout to a whopping 5000 seconds!!!\nAs for the Rust implementation, I set it up to retry the request if no response was received within 4 minutes.\nwhile is_success == false { let count = client.get(\u0026#34;URL\u0026#34;).timeout(std::time::Duration::new(240, 0)).send().await; match count { Ok(expr) =\u0026gt; { is_success = true; } Err(_) =\u0026gt; { println!(\u0026#34;timed out, retrying\u0026#34;); continue; } } } Little did I know that this plan would lead to catastrophic consequences and bring production to a screeching halt.\nAfter implementing the changes, I ran the system and decided to take a well-deserved lunch break. Upon returning, I anxiously checked the logs, only to discover that the retries were indeed happening. Progress, albeit slow, was being made. However, my relief was short-lived. Half an hour later, an alarming message arrived, notifying me that production was unresponsive.\nTo my horror, I found the database in an unresponsive state. CPU usage and disk I/O bandwidth were pushed to their limits. It became apparent that the sheer size of our query required PostgreSQL to temporarily store data on the disk and read it back later. With my incessant retries, without cancelling prior requests, the database was bombarded with an astronomical number of reads and writes, leading to its downfall.\nSwiftly, I terminated all connections to the database, restoring order and stability. However, the bandwidth for that day was a casualty of the chaos. Frustration bubbled within me, yet I was determined to return to the drawing board once more.\nDiscovering Success: The Silver Lining # After enduring a grueling amount of trial and error, I finally devised a new plan to tackle the persistent timeout issue. The key was to segment the request into a maximum of one month per request. For example, if a user requested a five-month-long report, I would split it into five separate one-month requests and send them to the database.\nfn split_date_range(start_date: \u0026amp;str, end_date: \u0026amp;str) -\u0026gt; Vec\u0026lt;DateRange\u0026gt; { let start_date = NaiveDate::parse_from_str(start_date, \u0026#34;%Y-%m-%d\u0026#34;).unwrap(); let end_date = NaiveDate::parse_from_str(end_date, \u0026#34;%Y-%m-%d\u0026#34;).unwrap(); let mut split_dates = Vec::new(); let mut current_date = start_date; while current_date \u0026lt; end_date { let next_month_start = current_date .with_day(1) .unwrap() .with_month(current_date.month() + 1) .unwrap(); let split_end_date = if next_month_start \u0026lt;= end_date { next_month_start - chrono::Duration::days(1) } else { end_date }; let date_range = DateRange { start_date: current_date.to_string(), end_date: split_end_date.to_string(), }; split_dates.push(date_range); current_date = next_month_start; } split_dates } But why this segmentation, you may wonder?\nWell, the root cause of the timeouts lay in my initial approach of requesting the exact count of rows for each individual request. This proved to be a costly operation in PostgreSQL, taking longer than fetching the actual data itself. To make matters worse, I was repeatedly requesting the count, multiplying the impact. It dawned on me that by reducing the amount of data requested, the count operation would be significantly faster. It was a hypothesis worth testing.\nWith cautious optimism, I implemented the segmentation approach, and lo and behold, it worked! Even when requesting data from the very beginning of the app until the current date, the system performed admirably. Granted, it may still be considered slow, but compared to the previous state of perpetual timeouts, delivering five months\u0026rsquo; worth of data in just two minutes was a considerable improvement. From infinity to two minutes, I\u0026rsquo;d say that\u0026rsquo;s more than decent progress.\nWith my newfound solution in hand, I confidently deployed and tested it in the production environment. And you know what? It worked flawlessly. After an arduous journey filled with countless setbacks, I had finally found the light at the end of the tunnel.\nConclusion # It has been a puzzling battle, but one that was fought valiantly, if I may say so myself. This blog post only scratches the surface of what I learned throughout this feature request journey. In reality, the lessons were far more extensive and intricate. However, fear not, for I plan to delve into those details in a separate post, providing a deeper exploration of the knowledge gained. For now, let this post serve as an amusing tale of taming the elephant.\nIn hindsight, it\u0026rsquo;s clear that better table design could have avoided many of the challenges we faced. But compromises are often inevitable, and requirements tend to change. If only we had known the precise report style from the very beginning, we could have designed more performant tables. Alas, what\u0026rsquo;s done is done, and a total rewrite is rarely the optimal solution. Sometimes, we must repair a flat tire on a moving car, a difficult task indeed. Yet, with determination and effort, we can overcome the hurdles and find a way forward.\nIn the end, this experience has been a valuable lesson in problem-solving and resilience. It reminded me that even in the face of seemingly insurmountable obstacles, there is always a path to a solution. It may require creativity, perseverance, and a healthy dose of humor, but with these tools in hand, we can triumph over the most formidable challenges. So, onward we go, armed with newfound knowledge and a spirit ready to conquer whatever the future may bring.\n","date":"10 July 2023","externalUrl":null,"permalink":"/posts/the-quest-to-export-report-detail-with-range/","section":"Posts","summary":"The dramatic story (not really) of coping with postgre performance","title":"The quest to export report detail with range","type":"posts"},{"content":"SWE, heavy on mobile dev, so-so in the other \u0026lsquo;regular\u0026rsquo; dev.\nMost of my work still leans toward mobile, but these days I wear more hats.\nI pursue graphics programming in my own time, not just a passing interest, but a path I actively invest in.\nWhat I do # I build and maintain software across platforms mostly mobile, but also backend and supporting systems when needed.\nOutside of work, I pursue a couple of paths more deliberately:\nGame development, prototyping ideas, joining jams, and slowly shaping a small commercial project. Graphics programming, something I pursue in my own time. Right now that means learning Vulkan, experimenting with rendering pipelines, and building a basic renderer. Philosophy # I care about the process as much as the product.\nI like understanding how things work under the hood, memory, data, execution, the stuff the CPU actually does. That doesn’t mean I reject abstraction.\nStill, I find value in knowing what’s really happening. Even if I’m not optimizing every bit of memory, I like having that mental model.\nI also believe that someone should have a handcrafted environment that works for them.\nI use:\nNeovim Arch Linux + Sway I type on a split keyboard using AKL (Night) I want tools that feel like limbs, responsive, precise, frictionless.\nI’m not saying everyone needs to use Vim. I’m saying you should explore deeply enough to know what works for you. Use VSCode? Great. Just know why.\nI’m also forcing myself to get fluent with Windows, even though I hate it.\nI want to be coconut oiled, even in the mud.\nPlus it seems that the graphic / game dev side of things mostly happens there.\nHobbies # Gaming\u0026rsquo;s always been part of my life, but my taste evolved.\nPS1/PS2 days: bought games by their covers. Pure vibes. Teen years: obsessed with AAA. Back when big publishers rarely missed. Later: fell into the Dota / CS hole. Thousands of hours. Now: back to vibe-based games that look cool. I also read a lot of manga and manhwa.\nYes, I know my taste is shallow.\nA deep, well-written story with complex characters? Fine, sure.\nBut the 546th 6/10 isekai with an overpowered protagonist? Good shit.\nAnother power fantasy manhwa with recycled concepts? We eating good tonight.\nI\u0026rsquo;m also a fan of Uma Musume.\nThe horses just speak to me.\nMy head hurts from nodding and trueing. These horses are literally me.\nAnd then there\u0026rsquo;s Touhou. I don\u0026rsquo;t fully know why I love it.\nMaybe because it\u0026rsquo;s the first open source game with a million forks, some of which aren\u0026rsquo;t even games anymore.\nAlso: Akatsuki Records is my favorite circle.\nFollowed closely by Tumeneco.\nWhat This Site Is # This site is a small corner of the internet for the things I build and think about.\nIf you\u0026rsquo;re into custom tooling, low-level graphics, or discussing how the protagonist farmed aura for the 50th time in chapter 50 - we might get along.\nYou can reach me at GitHub or contact@posei.me.\n","externalUrl":null,"permalink":"/about/","section":"Posei Site","summary":"\u003cp\u003eSWE, heavy on mobile dev, so-so in the other \u0026lsquo;regular\u0026rsquo; dev.\u003cbr\u003e\nMost of my work still leans toward mobile, but these days I wear more hats.\u003c/p\u003e\n\u003cp\u003eI pursue graphics programming in my own time, not just a passing interest, but a path I actively invest in.\u003c/p\u003e","title":"About","type":"page"},{"content":"","externalUrl":null,"permalink":"/authors/","section":"Authors","summary":"","title":"Authors","type":"authors"},{"content":"","externalUrl":null,"permalink":"/series/","section":"Series","summary":"","title":"Series","type":"series"}]